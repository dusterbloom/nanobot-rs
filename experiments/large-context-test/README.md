# Large-Context Multi-Agent Test

Reproduce the Lex Fridman transcript analysis pattern with nanobot using only local models.

## Objective

Test nanobot's ability to:
1. Process a large CSV (~100K characters / ~88 papers)
2. Identify relevant subsets (top 5 authors)
3. Spawn parallel subagents for extraction
4. Aggregate results into a coherent summary

All using **local models only** on a single RTX3090 (24GB VRAM).

## Model Allocation

| Role | Model | VRAM | Port | Purpose |
|------|-------|------|------|---------|
| Main Agent | Ministral-8B | ~5GB | 8080 | Reasoning, coordination |
| Subagent | Ministral-3-3B | ~2GB | 8083 | Task execution |

**Total VRAM: ~7GB** (17GB headroom for KV cache)

Both models are **non-thinking** (no reasoning_content output). Subagent fix applied for strict Jinja templates.

## Known Issues (from test run 2026-02-16)

### Subagent Jinja Template Error (FIXED)
Strict Jinja chat templates (Ministral, some Llama) fail in subagent mode because subagent uses `role: "tool"` for tool results.

**Fix Applied**: `subagent.rs` now calls `repair_for_strict_alternation()` for local models to convert tool messages to user messages.

### Qwen2.5-VL Notes
- Qwen2.5-VL is a vision-language model but works fine for text-only tasks
- Uses relaxed chat template (no strict alternation required)
- 7B parameters, good balance of capability and speed

## Quick Start

```bash
# 1. Fetch ArXiv data (~5 min)
python3 data/fetch_arxiv.py

# 2. Generate ground truth
python3 scripts/ground_truth.py

# 3. Start llama-server cluster
./scripts/setup.sh start

# 4. Run the test
./scripts/run_test.sh

# 5. Validate results
python3 scripts/validate.py results/nanobot_output_*.txt
```

## Directory Structure

```
large-context-test/
├── README.md
├── config.local.json      # Nanobot config for local models
├── data/
│   ├── fetch_arxiv.py     # Data collection script
│   └── arxiv_papers.csv   # Generated dataset (~500 papers)
├── prompts/
│   └── test_prompt.txt    # The multi-step task
├── scripts/
│   ├── setup.sh           # Launch llama-server cluster
│   ├── run_test.sh        # Full test pipeline
│   ├── ground_truth.py    # Generate expected answers
│   └── validate.py        # Compare output to ground truth
└── results/
    ├── ground_truth.json  # Expected results
    ├── nanobot_output_*.txt  # Actual output
    └── validation_results.json
```

## Ground Truth

The ground truth is generated by:
1. Counting papers per author to find top 10
2. Searching for keywords: `emergent capabilities`, `emergence`, `scaling laws`, `phase transition`
3. Recording which authors discuss these topics and what they say

This allows quantitative validation:
- **Author coverage**: Did nanobot identify the same top 10?
- **Keyword coverage**: Did it find the relevant discussions?
- **Accuracy**: Are the extracted insights correct?

## Metrics Collected

| Metric | How |
|--------|-----|
| Wall clock time | `time` command |
| Author coverage | % of ground-truth authors mentioned |
| Keyword coverage | % of ground-truth findings reflected |
| Emergence detection | Binary: did it discuss the topic? |

## Success Criteria

- **PASS**: Author coverage > 70% AND emergence topic discussed
- **FAIL**: Otherwise

## Alternative Model Configurations

### Single-Server Mode
If you only want to test with one model:

```bash
# Edit setup.sh to only start one server
# Use Qwen2.5 for everything (main + subagent + delegation)
declare -A MODELS=(
    ["main"]="$MODELS_DIR/Qwen2.5-VL-7B-Instruct-Q4_K_M.gguf:8080:16384"
)
```

### Fallback: Ministral-8B
If Qwen2.5 doesn't work well, Ministral-8B is a capable alternative (requires the subagent fix).

## Troubleshooting

### "Model not found"
Check that GGUF files exist in `~/models/` or `~/.nanobot/models/`

### "CUDA out of memory"
Reduce context sizes in `setup.sh` or use smaller models

### "Server not ready"
Check logs: `cat logs/main.log`, `cat logs/subagent.log`, etc.

### Nanobot uses wrong model
Ensure `config.local.json` is copied to `~/.nanobot/config.json` or set `NANOBOT_CONFIG`

## References

- Original test: Lex Fridman podcast transcript analysis (60M chars, 320 episodes)
- Target: Reproduce pattern with local models
- Expected advantage over ReAct: 10-20x cost reduction via:
  - Parallel subagents (not sequential)
  - Symbolic variable passing (not read/write context)
  - Strategic model tiering (big brain + small hands)
